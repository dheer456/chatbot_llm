
Three popular ways of building chatbots with LLM, focused on a particular use case or on a dataset:

Retrieval Augmented Generation (RAG): The AI gives responses based on information retrieved from a embeddings dataset (vector store), making the responses much more specialized as per the data that is stored. RAG combines a retrieval module with a generative model to fetch relevant information from a large document database or knowledge base before generating a response.

Finetuning the LLM: Fine-tuning refers to adapting pre-trained language models to specific tasks or domains by training them further on a curated dataset. In chatbot building, fine-tuning helps tailor the model’s behavior to the desired use case, improving its accuracy in providing relevant responses.


Prompt Engineering: Prompt engineering involves crafting specific input prompts to guide a language model’s behavior, allowing you to extract more precise responses or direct the chatbot to behave in a particular manner. Prompt engineering is critical for both fine-tuned and generative models, especially when they are used in open-ended tasks like conversation.
